//
//  CameraManager.swift
//  Aptos Passport KYC
//
//  Created by Harold on 2025/7/17.
//

import UIKit
import AVFoundation
import Vision
import CoreImage

// MARK: - CameraManager â€”â€” è´Ÿè´£é‡‡é›†åƒç´ ç¼“å†²å¹¶å‘å¸ƒ
final class CameraManager: NSObject, ObservableObject {
    
    @Published var latestPixelBuffer: CVPixelBuffer?     // æœ€æ–°è§†é¢‘å¸§
    @Published var session = AVCaptureSession()          // å…¬å¼€å‘å¸ƒ session
    @Published var isRunning = false                     // æ‘„åƒå¤´è¿è¡ŒçŠ¶æ€
    
    private let videoOutput = AVCaptureVideoDataOutput()
    private let sessionQueue = DispatchQueue(label: "cam.queue", qos: .userInitiated)
    
    override init() {
        super.init()
        setupCamera()
    }
    
    /// è®¾ç½®æ‘„åƒå¤´é…ç½®
    private func setupCamera() {
        session.beginConfiguration()
        session.sessionPreset = .vga640x480
        
        guard
            let device = AVCaptureDevice.default(.builtInWideAngleCamera,
                                                 for: .video, position: .front),
            let input = try? AVCaptureDeviceInput(device: device),
            session.canAddInput(input)
        else {
            Swift.print("âŒ front camera unavailable")
            session.commitConfiguration()
            return
        }
        
        session.addInput(input)
        
        videoOutput.setSampleBufferDelegate(self, queue: sessionQueue)
        guard session.canAddOutput(videoOutput) else {
            session.commitConfiguration()
            return
        }
        
        session.addOutput(videoOutput)
        videoOutput.connections.first?.videoOrientation = .portrait
        
        session.commitConfiguration()
    }
    
    /// å¯åŠ¨æ‘„åƒå¤´
    func start() {
        guard !isRunning else { 
            #if DEBUG
            Swift.print("ğŸ“¹ [CameraManager] æ‘„åƒå¤´å·²åœ¨è¿è¡Œï¼Œè·³è¿‡å¯åŠ¨")
            #endif
            return 
        }
        
        #if DEBUG
        Swift.print("ğŸ“¹ [CameraManager] å¼€å§‹å¯åŠ¨æ‘„åƒå¤´")
        #endif
        
        sessionQueue.async {
            #if DEBUG
            Swift.print("ğŸ“¹ [CameraManager] åœ¨sessioné˜Ÿåˆ—ä¸­å¯åŠ¨æ‘„åƒå¤´")
            #endif
            
            self.session.startRunning()
            
            DispatchQueue.main.async {
                self.isRunning = true
                #if DEBUG
                Swift.print("ğŸ“¹ [CameraManager] æ‘„åƒå¤´å¯åŠ¨å®Œæˆï¼ŒçŠ¶æ€å·²æ›´æ–°")
                #endif
            }
        }
    }
    
    /// åœæ­¢æ‘„åƒå¤´
    func stop() {
        guard isRunning else { 
            #if DEBUG
            Swift.print("ğŸ“¹ [CameraManager] æ‘„åƒå¤´å·²åœæ­¢ï¼Œè·³è¿‡åœæ­¢æ“ä½œ")
            #endif
            return 
        }
        
        #if DEBUG
        Swift.print("ğŸ“¹ [CameraManager] å¼€å§‹åœæ­¢æ‘„åƒå¤´")
        #endif
        
        sessionQueue.async {
            #if DEBUG
            Swift.print("ğŸ“¹ [CameraManager] åœ¨sessioné˜Ÿåˆ—ä¸­åœæ­¢æ‘„åƒå¤´")
            #endif
            
            self.session.stopRunning()
            
            DispatchQueue.main.async {
                self.isRunning = false
                #if DEBUG
                Swift.print("ğŸ“¹ [CameraManager] æ‘„åƒå¤´åœæ­¢å®Œæˆï¼ŒçŠ¶æ€å·²æ›´æ–°")
                #endif
            }
        }
    }
    
    /// ä»å½“å‰å¸§ä¸­æå–äººè„¸å›¾åƒ
    func extractFaceImage(from pixelBuffer: CVPixelBuffer) -> UIImage? {
        #if DEBUG
        Swift.print("ğŸ” [CameraManager] å¼€å§‹ä»åƒç´ ç¼“å†²åŒºæå–äººè„¸å›¾åƒ")
        #endif
        
        var detector = FaceDetector()
        let detection = detector.detect(in: pixelBuffer)
        
        #if DEBUG
        Swift.print("ğŸ” [CameraManager] äººè„¸æ£€æµ‹ç»“æœ: æ•°é‡=\(detection.count), ç¬¬ä¸€ä¸ªæ¡†=\(detection.firstBox?.debugDescription ?? "æ— ")")
        #endif
        
        guard let box = detection.firstBox else {
            #if DEBUG
            Swift.print("ğŸ” [CameraManager] æœªæ£€æµ‹åˆ°äººè„¸ï¼Œè¿”å›nil")
            #endif
            return nil
        }
        
        // è®¡ç®—åƒç´  ROI (ç¿»è½¬ Y è½´)
        let w = CGFloat(CVPixelBufferGetWidth(pixelBuffer))
        let h = CGFloat(CVPixelBufferGetHeight(pixelBuffer))
        let rect = CGRect(x: box.origin.x * w,
                          y: (1 - box.origin.y - box.height) * h,
                          width: box.width * w,
                          height: box.height * h)
        
        #if DEBUG
        Swift.print("ğŸ” [CameraManager] åƒç´ ç¼“å†²åŒºå°ºå¯¸: \(w)x\(h), äººè„¸æ¡†: \(rect)")
        #endif
        
        // éªŒè¯è¾¹ç•Œå€¼çš„æœ‰æ•ˆæ€§
        guard rect.width > 0 && rect.height > 0 && 
              rect.origin.x >= 0 && rect.origin.y >= 0 && 
              rect.maxX <= w && rect.maxY <= h else {
            #if DEBUG
            Swift.print("ğŸ” [CameraManager] äººè„¸æ¡†è¾¹ç•Œæ— æ•ˆï¼Œè¿”å›nil")
            #endif
            return nil
        }
        
        // ä» pixelBuffer è£å‡º ROI -> UIImage
        let ciImage = CIImage(cvPixelBuffer: pixelBuffer).cropped(to: rect)
        let context = CIContext()
        
        #if DEBUG
        Swift.print("ğŸ” [CameraManager] åˆ›å»ºCIå›¾åƒï¼Œextent: \(ciImage.extent)")
        #endif
        
        // ç¡®ä¿ CIImage çš„ extent æœ‰æ•ˆ
        guard !ciImage.extent.isEmpty else {
            #if DEBUG
            Swift.print("ğŸ” [CameraManager] CIå›¾åƒextentä¸ºç©ºï¼Œè¿”å›nil")
            #endif
            return nil
        }
        
        guard let cgFace = context.createCGImage(ciImage, from: ciImage.extent) else {
            #if DEBUG
            Swift.print("ğŸ” [CameraManager] åˆ›å»ºCGå›¾åƒå¤±è´¥ï¼Œè¿”å›nil")
            #endif
            return nil
        }
        
        #if DEBUG
        Swift.print("ğŸ” [CameraManager] æˆåŠŸæå–äººè„¸å›¾åƒï¼Œå°ºå¯¸: \(cgFace.width)x\(cgFace.height)")
        #endif
        
        return UIImage(cgImage: cgFace)
    }
}

// MARK: - AVCaptureVideoDataOutputSampleBufferDelegate
extension CameraManager: AVCaptureVideoDataOutputSampleBufferDelegate {
    func captureOutput(_ output: AVCaptureOutput,
                       didOutput sampleBuffer: CMSampleBuffer,
                       from connection: AVCaptureConnection) {
        #if DEBUG
        let timestamp = CMSampleBufferGetPresentationTimeStamp(sampleBuffer)
        Swift.print("ğŸ“¹ [CameraManager] æ¥æ”¶åˆ°æ–°è§†é¢‘å¸§ï¼Œæ—¶é—´æˆ³: \(timestamp.seconds)")
        #endif
        
        DispatchQueue.main.async {
            self.latestPixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer)
            #if DEBUG
            Swift.print("ğŸ“¹ [CameraManager] è§†é¢‘å¸§å·²æ›´æ–°åˆ°ä¸»çº¿ç¨‹")
            #endif
        }
    }
}

// MARK: - FaceDetector â€”â€” åªè´Ÿè´£è¿”å›äººè„¸æ¡†æ•°é‡ä¸ç¬¬ä¸€ä¸ªæ¡†
struct FaceDetector {
    
    private let request = VNDetectFaceRectanglesRequest()
    
    mutating func detect(in pixelBuffer: CVPixelBuffer) -> (count: Int, firstBox: CGRect?) {
        #if DEBUG
        Swift.print("ğŸ¯ [FaceDetector] å¼€å§‹äººè„¸æ£€æµ‹")
        #endif
        
        let handler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer,
                                            orientation: .leftMirrored)
        do {
            try handler.perform([request])
            let faces = request.results as? [VNFaceObservation] ?? []
            
            #if DEBUG
            Swift.print("ğŸ¯ [FaceDetector] æ£€æµ‹å®Œæˆï¼Œäººè„¸æ•°é‡: \(faces.count)")
            if let firstFace = faces.first {
                Swift.print("ğŸ¯ [FaceDetector] ç¬¬ä¸€ä¸ªäººè„¸æ¡†: \(firstFace.boundingBox), ç½®ä¿¡åº¦: \(firstFace.confidence)")
            }
            #endif
            
            return (faces.count, faces.first?.boundingBox)
        } catch {
            #if DEBUG
            Swift.print("ğŸ¯ [FaceDetector] Visionæ£€æµ‹é”™è¯¯: \(error)")
            #endif
            return (0, nil)
        }
    }
}
